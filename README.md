# ğŸ§¹ AutoKlean - Autonomous Data Preparation Engine

<div align="center">

![AutoKlean](https://img.shields.io/badge/AutoKlean-v2.0-brightgreen?style=for-the-badge)
![React](https://img.shields.io/badge/React-19.2-61DAFB?style=for-the-badge&logo=react)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=for-the-badge&logo=fastapi)
![Node.js](https://img.shields.io/badge/Node.js-18+-339933?style=for-the-badge&logo=node.js)
![MongoDB](https://img.shields.io/badge/MongoDB-7.0-47A248?style=for-the-badge&logo=mongodb)

**A full-stack web application for autonomous data cleaning, profiling, and ML-ready dataset preparation.**

[Quick Start](#-quick-start) â€¢ [Features](#-features) â€¢ [Architecture](#-architecture) â€¢ [API Reference](#-api-reference)

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [How It Works](#-how-it-works)
- [API Reference](#-api-reference)
- [Configuration Options](#-configuration-options)

---

## ğŸ¯ Overview

**AutoKlean** is an intelligent data preparation platform that automates the tedious process of cleaning and preparing raw datasets for machine learning. Simply upload your CSV or Excel file, and AutoKlean will:

1. **Automatically profile** your data (detect types, find missing values, identify outliers)
2. **Clean and transform** data using intelligent pipelines
3. **Generate train/test splits** or K-fold cross-validation sets
4. **Export ML-ready datasets** with full transformation logs

### Why AutoKlean?

| Traditional Approach | With AutoKlean |
|---------------------|----------------|
| Hours of manual data inspection | Instant automated profiling |
| Custom scripts for each dataset | One-click intelligent cleaning |
| Error-prone manual splitting | Configurable train/test & K-fold splits |
| No reproducibility | Generated Python code for every pipeline |

---

## âœ¨ Features

### Core Capabilities

| Feature | Description |
|---------|-------------|
| ğŸ“¤ **Smart Upload** | Drag-and-drop CSV/Excel files with instant validation |
| ğŸ” **Auto-Profiling** | Detects data types, missing values, outliers, and quality metrics |
| ğŸ§¹ **Intelligent Cleaning** | Handles missing data, normalizes values, encodes categories |
| âœ‚ï¸ **Train/Test Split** | Configurable 0-100% split ratio with random seed control |
| ğŸ”„ **K-Fold CV** | Generate 2-20 fold cross-validation sets |
| ğŸ“Š **Quality Metrics** | Completeness, uniqueness, and distribution analysis |
| ğŸ **Python Export** | Auto-generated reproducible Python code |
| ğŸ“¦ **Batch Download** | Download all outputs as ZIP |

### Pipeline Operations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE OPTIONS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Clean Only        â†’ cleaned.csv                         â”‚
â”‚  2. Split Only        â†’ train.csv + test.csv                â”‚
â”‚  3. Clean + Split     â†’ cleaned_train.csv + cleaned_test.csvâ”‚
â”‚  4. K-Fold CV         â†’ fold_1_train.csv ... fold_k_val.csv â”‚
â”‚  5. Clean + K-Fold    â†’ cleaned K-fold files                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cleaning Operations

- **Missing Value Imputation**: Mean, median, mode, or KNN-based
- **Outlier Removal**: IsolationForest-based detection
- **Feature Scaling**: StandardScaler, MinMaxScaler, RobustScaler
- **Encoding**: Label encoding, one-hot encoding
- **Type Coercion**: Automatic numeric/datetime/boolean detection
- **Schema Cleaning**: Column renaming to snake_case

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLIENT (Browser)                          â”‚
â”‚                    React 19 + Vite + TailwindCSS                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ HTTP (REST API)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BACKEND (Node.js/Express)                    â”‚
â”‚  â€¢ File Upload (Multer)     â€¢ Dataset Management                  â”‚
â”‚  â€¢ MongoDB Integration      â€¢ Pipeline Orchestration              â”‚
â”‚  â€¢ ZIP Generation           â€¢ Static File Serving                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ HTTP (Internal API)
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ML SERVICE (Python/FastAPI)                  â”‚
â”‚  â€¢ Data Profiling           â€¢ Auto-Cleaning Pipeline              â”‚
â”‚  â€¢ Train/Test Splitting     â€¢ K-Fold Cross-Validation             â”‚
â”‚  â€¢ Outlier Detection        â€¢ Feature Engineering                 â”‚
â”‚  â€¢ Python Code Generation   â€¢ Transformation Logging              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STORAGE                                   â”‚
â”‚  â€¢ MongoDB (metadata, profiles, pipeline runs)                    â”‚
â”‚  â€¢ File System (uploaded & processed datasets)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Frontend** | React 19, Vite 7, TailwindCSS 4 | Modern reactive UI |
| **Backend** | Node.js, Express 5, Mongoose 9 | REST API & file handling |
| **ML Service** | Python 3.12, FastAPI, scikit-learn | Data processing & ML ops |
| **Database** | MongoDB 7 | Document storage |
| **Styling** | TailwindCSS, Lucide Icons | UI components |

---

## ğŸ“ Project Structure

```
Autonomous_Data_Cleaning/
â”œâ”€â”€ frontend/                    # React Frontend Application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx             # Main application component
â”‚   â”‚   â”œâ”€â”€ App.css             # Custom styles
â”‚   â”‚   â””â”€â”€ main.jsx            # Entry point
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ backend/                     # Node.js Backend API
â”‚   â”œâ”€â”€ server.js               # Express server entry
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ datasetRoutes.js    # API endpoints
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ Dataset.js          # Dataset schema
â”‚   â”‚   â”œâ”€â”€ Profile.js          # Profile schema
â”‚   â”‚   â””â”€â”€ PipelineRun.js      # Pipeline run schema
â”‚   â”œâ”€â”€ uploads/                # Uploaded & processed files
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ ml-service/                  # Python ML Service
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ test_pipeline.py        # Test scripts
â”‚
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ QUICK_START.md              # Setup guide
â”œâ”€â”€ PIPELINE_OPERATIONS.md      # Feature documentation
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md   # Technical details
â””â”€â”€ TESTING_GUIDE.md            # Testing checklist
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** v18+ 
- **Python** 3.10+
- **MongoDB** running on `localhost:27017`

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/Autonomous_Data_Cleaning.git
cd Autonomous_Data_Cleaning

# Install backend dependencies
cd backend
npm install

# Install frontend dependencies
cd ../frontend
npm install

# Install Python dependencies
cd ../ml-service
pip install -r requirements.txt
```

### Running the Application

Open **3 terminals** and run:

```bash
# Terminal 1: Backend (Port 5000)
cd backend
node server.js

# Terminal 2: ML Service (Port 8000)
cd ml-service
python main.py

# Terminal 3: Frontend (Port 5173)
cd frontend
npm run dev
```

### Access the Application

Open your browser and navigate to: **http://localhost:5173**

---

## âš™ï¸ How It Works

### 1. Upload Dataset
```
User uploads CSV/Excel â†’ Backend saves file â†’ ML Service profiles data
                                            â†“
                              Returns: column types, missing %, stats
```

### 2. Configure Pipeline
```
User selects options:
  â”œâ”€â”€ Train/Test Split Ratio (0-100%)
  â”œâ”€â”€ K-Folds for Cross-Validation (0-20)
  â”œâ”€â”€ Remove Outliers (on/off)
  â”œâ”€â”€ Impute Missing Values (on/off)
  â””â”€â”€ Normalize Features (on/off)
```

### 3. Execute Pipeline
```
Backend receives config â†’ Calls ML Service /auto-clean endpoint
                                    â†“
ML Service performs:
  1. Schema cleaning (column names, type coercion)
  2. Semantic cleaning (missing tokens, standardization)
  3. Outlier removal (if enabled)
  4. Missing value imputation (if enabled)
  5. Feature normalization (if enabled)
  6. Train/Test split or K-Fold generation
  7. Save processed files
  8. Generate Python code
                                    â†“
Returns: file paths, transformation log, Python code
```

### 4. Download Results
```
User downloads:
  â”œâ”€â”€ cleaned.csv (cleaned dataset)
  â”œâ”€â”€ train.csv + test.csv (if split enabled)
  â”œâ”€â”€ fold_X_train.csv + fold_X_val.csv (if K-fold enabled)
  â””â”€â”€ pipeline_code.py (reproducible Python script)
```

---

## ğŸ“¡ API Reference

### Backend Endpoints (Port 5000)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/datasets/upload` | Upload file & auto-profile |
| `GET` | `/api/datasets/:id/profile` | Get dataset profile |
| `POST` | `/api/datasets/:id/clean` | Run cleaning pipeline |
| `GET` | `/uploads/:filename` | Download processed files |

### ML Service Endpoints (Port 8000)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/profile` | Profile a dataset |
| `POST` | `/auto-clean` | Run full cleaning pipeline |
| `GET` | `/docs` | Swagger API documentation |

### Example: Run Pipeline

```bash
curl -X POST http://localhost:5000/api/datasets/DATASET_ID/clean \
  -H "Content-Type: application/json" \
  -d '{
    "splitRatio": 80,
    "kFolds": 5,
    "removeOutliers": true,
    "imputeMissing": true,
    "normalizeFeatures": true
  }'
```

---

## ğŸ›ï¸ Configuration Options

| Option | Type | Range | Default | Description |
|--------|------|-------|---------|-------------|
| `splitRatio` | number | 0-100 | 0 | Train set percentage (0 = no split) |
| `kFolds` | number | 0-20 | 0 | Number of CV folds (0 = disabled) |
| `epochs` | number | 0+ | 0 | Reserved for future use |
| `removeOutliers` | boolean | - | false | Enable outlier removal |
| `imputeMissing` | boolean | - | false | Enable missing value imputation |
| `normalizeFeatures` | boolean | - | false | Enable feature scaling |

---

## ğŸ–¼ï¸ UI Features

### Main Interface
- **Dark theme** with neon green (#ccff00) accents
- **Drag-and-drop** file upload zone
- **Real-time** processing logs in terminal-style panel
- **Bento grid** layout for data visualization

### Pipeline Config Panel
- **Number input + slider** for precise train/test split control
- **Increment/decrement buttons** with number input for K-folds
- **Toggle switches** for cleaning options (all off by default)

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

<div align="center">

**Built with â¤ï¸ for the ML community**

</div>
